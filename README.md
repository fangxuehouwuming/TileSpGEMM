# TileSpGEMM

**TileSpGEMM** is an open source code that uses a tiled structure to optimize general sparse matrix-matrix multiplication (SpGEMM) on GPUs.

本仓库是对 TileSpGEMM 的复现。

---

## 安装

Linux + CUDA GPU driver + the nvcc CUDA compiler + cuSPARSE library

其中，CUDA 相关的环境均包含在 CUDA Toolkit

## 运行

1. Set CUDA path in the Makefile

   `CUDA_INSTALL_PATH = /usr/local/cuda-11.0`

2. Run command 'make' and generate an executable file 'test' for double precision.

   > **make**

3. Run SpGEMM code on matrix data with auto-tuning in double precision. The GPU compilation takes an optional d=<gpu-device, e.g., 0> parameter that specifies the GPU device to run if multiple GPU devices are available at the same time, and another optional aat=<transpose, e.g., 0> parameter that means computing C = A^2 (-aat 0) or C = AA^T (-aat 1)).

   > **$ ./test -d 0 -aat 0 <path/to/dataset/mtx>**

   > **$ ./test -d 0 -aat 1 <path/to/dataset/mtx>**

## 输出信息

Lines 1-2 output the input matrix's information including the path of matrix file, The number of rows, columns and nonzeros.

Line 3 prints the file loading time (in seconds).

Line 4 prints the size of tile used in our TileSpGEMM algorithm.

Line 5 prints the number of floating point operations during the multiplication.

Line 6 prints the runtime of transforming the input matrix from the CSR format to our tiled data structure (in millisec- onds) (Figure 12 in our paper).

Line 7 prints TileSpGEMM data structure's space consump- tion (in million bytes) (Figure 11 in our paper).

Lines 8-14 print execution time (in milliseconds) of the three algorithm steps and all memory allocation on CPU and GPU (Figure 10 in our paper).

Line 15 prints the number of tiles of the resulting matrix C.

Line 16 prints the number of nonzeros of the resulting matrix C.

Line 17 prints TileSpGEMM runtime (in milliseconds) and performance (in GFlOPs) (Figures 6 and 7 in our paper).

Line 18 prints the checking result after comparing our output with the one generated by cuSPARSE.
